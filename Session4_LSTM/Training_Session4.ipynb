{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training_Session4",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyZLizSrtrvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3826ec48-e654-44a1-b3b1-86cda5ab8213"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "from os import listdir\n",
        "from os.path import isfile\n",
        "import re\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJy4bBU7HCHF"
      },
      "source": [
        "MAX_DOC_LENGTH = 500\n",
        "NUM_CLASSES = 20\n",
        "unknown_ID = 1\n",
        "padding_ID = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Cckd7UYANeO"
      },
      "source": [
        "#Build data files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAXGNgPyvha1"
      },
      "source": [
        "def gen_data_and_vocab():\n",
        "  def colect_data_from(parent_path,newsgroup_list,word_count = None):\n",
        "    data = []\n",
        "    for group_id, newsgroup in enumerate(newsgroup_list):\n",
        "        label = group_id\n",
        "        dir_path = parent_path + '/' + newsgroup + '/'\n",
        "        files = [(filename, dir_path + filename) for filename in listdir(dir_path) if isfile(dir_path + filename)]\n",
        "        files.sort()\n",
        "        print('Processing: {}-{}'.format(group_id,newsgroup))\n",
        "        for filename, filepath in files:\n",
        "            with open(filepath,encoding = 'ISO-8859-1') as f:\n",
        "                text = f.read().lower()\n",
        "                words = re.split('\\W+',text)\n",
        "                if word_count is not None:\n",
        "                  for word in words:\n",
        "                    word_count[word] += 1\n",
        "                content = ' '.join(words)\n",
        "                assert len(content.splitlines()) == 1\n",
        "                data.append(str(label) + '<fff>' + filename + '<fff>' + content)\n",
        "    return data\n",
        "  word_count = defaultdict(int)\n",
        "\n",
        "  path = '../datasets/20news-bydate/'\n",
        "  dirs = [path + dir_name  for dir_name in listdir(path) if not isfile(path + dir_name)]\n",
        "  train_path, test_path = (dirs[0], dirs[1]) if 'train' in dirs[0] else (dirs[1], dirs[0])\n",
        "  newsgroup_list = [newsgroup for newsgroup in listdir(train_path)]\n",
        "  newsgroup_list.sort()\n",
        "\n",
        "  train_data = colect_data_from(parent_path = train_path,newsgroup_list = newsgroup_list, word_count = word_count )\n",
        "  vocab = [word for word, freq in zip(word_count.keys(),word_count.values()) if freq > 10]\n",
        "  vocab.sort()\n",
        "  with open('../datasets/20news-bydate/w2v/vocab-raw.txt','w') as f:\n",
        "    f.write('\\n'.join(vocab))\n",
        "  \n",
        "  test_data= colect_data_from(parent_path = test_path,newsgroup_list = newsgroup_list)\n",
        "  with open('../datasets/20news-bydate/w2v/20news-train-raw.txt','w') as f:\n",
        "    f.write('\\n'.join(train_data))\n",
        "\n",
        "  with open('../datasets/20news-bydate/w2v/20news-test-raw.txt','w') as f:\n",
        "    f.write('\\n'.join(train_data))\n",
        "  \n",
        "gen_data_and_vocab()                                 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY3GkSZq0bmc"
      },
      "source": [
        "def encode_data(data_path,vocab_path):\n",
        "  with open(vocab_path) as f:\n",
        "    vocab = dict([( word,word_ID + 2 ) for word_ID,word in enumerate(f.read().splitlines())])\n",
        "  with open(data_path) as f:\n",
        "    documents = [(line.split('<fff>')[0], line.split('<fff>')[1], line.split('<fff>')[2]) for line in f.read().splitlines()]\n",
        "  encoded_data = []\n",
        "  for document in documents:\n",
        "    label,doc_id,text = document\n",
        "    words = text.split()[:MAX_DOC_LENGTH]\n",
        "    sentence_length = len(words)\n",
        "    encoded_text = []\n",
        "    for word in words:\n",
        "      if word in vocab:\n",
        "        encoded_text.append(str(vocab[word]))\n",
        "      else:\n",
        "        encoded_text.append(str(unknown_ID))\n",
        "    if len(words) < MAX_DOC_LENGTH:\n",
        "      num_padding = MAX_DOC_LENGTH - len(words)\n",
        "      for _ in range(num_padding):\n",
        "        encoded_text.append(str(padding_ID))\n",
        "    encoded_data.append(str(label) + '<fff>' + str(doc_id) + '<fff>' + str(sentence_length) + '<fff>' + ' '.join(encoded_text))\n",
        "\n",
        "\n",
        "  dir_name = '/'.join(data_path.split('/')[:-1])\n",
        "  \n",
        "  file_name = '-'.join(data_path.split('/')[-1].split('-')[:-1])+'-encoded.txt'\n",
        "  with open(dir_name+ '/' + file_name , 'w') as f:\n",
        "    f.write('\\n'.join(encoded_data))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txqqkq3lpImD"
      },
      "source": [
        "encode_data('../datasets/20news-bydate/w2v/20news-train-raw.txt',\n",
        "            '../datasets/20news-bydate/w2v/vocab-raw.txt')\n",
        "encode_data('../datasets/20news-bydate/w2v/20news-test-raw.txt',\n",
        "            '../datasets/20news-bydate/w2v/vocab-raw.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MumhUx-AXj5"
      },
      "source": [
        "#RNN class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YhU846E-HNH"
      },
      "source": [
        "class RNN:\n",
        "  def __init__(self,vocab_size,embedding_size, lstm_size,batch_size):\n",
        "    self._vocab_size = vocab_size\n",
        "    self._embedding_size = embedding_size\n",
        "    self._lstm_size = lstm_size\n",
        "    self._batch_size = batch_size\n",
        "\n",
        "    self._data = tf.placeholder(tf.int32,shape = [batch_size,MAX_DOC_LENGTH])\n",
        "    self._labels = tf.placeholder(tf.int32,shape = [batch_size,])\n",
        "    self._sentence_lengths = tf.placeholder(tf.int32,shape = [batch_size,])\n",
        "    self._final_tokens = tf.placeholder(tf.int32,shape =  [batch_size,])\n",
        "\n",
        "  def embedding_layer(self,indices):\n",
        "    pretrained_vectors = []\n",
        "    pretrained_vectors.append(np.zeros(self._embedding_size))\n",
        "    np.random.seed(2021)\n",
        "\n",
        "    for _ in range(self._vocab_size + 1):\n",
        "      pretrained_vectors.append(np.random.normal(loc = 0., scale = 1., size = self._embedding_size))\n",
        "\n",
        "    pretrained_vectors = np.array(pretrained_vectors)\n",
        "\n",
        "    with tf.variable_scope('embedding', reuse = tf.AUTO_REUSE):\n",
        "      self._embedding_matrix = tf.get_variable(\n",
        "            name = 'embedding',\n",
        "            shape = (self._vocab_size + 2, self._embedding_size),\n",
        "            initializer = tf.constant_initializer(pretrained_vectors)\n",
        "      )\n",
        "\n",
        "    return tf.nn.embedding_lookup(self._embedding_matrix, indices)\n",
        "\n",
        "  def LSTM_layer(self,embeddings):\n",
        "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(self._lstm_size)\n",
        "    zero_state = tf.zeros(shape = (self._batch_size, self._lstm_size))\n",
        "    initial_state = tf.contrib.rnn.LSTMStateTuple(zero_state, zero_state)\n",
        "\n",
        "    lstm_inputs = tf.unstack(tf.transpose(embeddings, perm = [1, 0, 2]))\n",
        "    with tf.variable_scope('lstm', reuse = tf.AUTO_REUSE):\n",
        "      lstm_outputs, last_state = tf.nn.static_rnn (\n",
        "            cell = lstm_cell,\n",
        "            inputs = lstm_inputs,\n",
        "            initial_state = initial_state,\n",
        "            sequence_length = self._sentence_lengths\n",
        "      )\n",
        "\n",
        "    lstm_outputs = tf.unstack(tf.transpose(lstm_outputs, perm = [1, 0, 2]))\n",
        "    lstm_outputs = tf.concat(lstm_outputs, axis = 0)\n",
        "\n",
        "    mask = tf.sequence_mask(\n",
        "        lengths = self._sentence_lengths,\n",
        "        maxlen = MAX_DOC_LENGTH,\n",
        "        dtype = tf.float32\n",
        "    )\n",
        "\n",
        "    mask = tf.concat(tf.unstack(mask, axis = 0), axis = 0)\n",
        "    mask = tf.expand_dims(mask, -1)\n",
        "\n",
        "    lstm_outputs = mask * lstm_outputs\n",
        "    lstm_outputs_split = tf.split(lstm_outputs, num_or_size_splits = self._batch_size)\n",
        "    lstm_outputs_sum = tf.reduce_sum(lstm_outputs_split, axis = 1)\n",
        "    lstm_outputs_average = lstm_outputs_sum / tf.expand_dims(tf.cast(self._sentence_lengths, tf.float32), -1)\n",
        "\n",
        "    return lstm_outputs_average\n",
        "\n",
        "  def build_graph(self):\n",
        "    embeddings = self.embedding_layer(self._data)\n",
        "    lstm_outputs = self.LSTM_layer(embeddings)\n",
        "    with tf.variable_scope('final_layer_weights', reuse = tf.AUTO_REUSE):\n",
        "      weights = tf.get_variable(\n",
        "          name = 'final_layer_weights',\n",
        "          shape = (self._lstm_size, NUM_CLASSES),\n",
        "          initializer = tf.random_normal_initializer(seed = 2021)\n",
        "      )\n",
        "    with tf.variable_scope('final_layer_biases', reuse = tf.AUTO_REUSE):\n",
        "      biases = tf.get_variable(\n",
        "            name = 'final_layer_biases',\n",
        "            shape = (NUM_CLASSES),\n",
        "            initializer = tf.random_normal_initializer(seed = 2021)\n",
        "      )\n",
        "    logits = tf.matmul(lstm_outputs, weights) + biases\n",
        "\n",
        "    labels_one_hot = tf.one_hot(\n",
        "        indices = self._labels,\n",
        "        depth = NUM_CLASSES,\n",
        "        dtype = tf.float32\n",
        "    )\n",
        "\n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits(\n",
        "        labels = labels_one_hot,\n",
        "        logits = logits\n",
        "    )\n",
        "\n",
        "    loss = tf.reduce_mean(loss)\n",
        "\n",
        "    probs = tf.nn.softmax(logits)\n",
        "    predicted_labels = tf.argmax(probs, axis = 1)\n",
        "    predicted_labels = tf.squeeze(predicted_labels)\n",
        "\n",
        "    return predicted_labels, loss\n",
        "  def trainer(self,loss,learning_rate):\n",
        "    with tf.variable_scope('optimizer', reuse = tf.AUTO_REUSE):\n",
        "      train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "      return train_op\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4XLrLNjREIB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2354fa3-70f6-464d-e6b2-3f390a14f9c0"
      },
      "source": [
        "\n",
        "with open('../datasets/20news-bydate/w2v/vocab-raw.txt',encoding = 'ISO-8859-1') as f:\n",
        "  vocab_size = len(f.read().splitlines())\n",
        "\n",
        "tf.random.set_random_seed(2021)\n",
        "rnn = RNN(\n",
        "  vocab_size = vocab_size,\n",
        "  embedding_size = 300,\n",
        "  lstm_size = 50,\n",
        "  batch_size = 50\n",
        ")\n",
        "\n",
        "predicted_labels , loss = rnn.build_graph()\n",
        "train_op = rnn.trainer(loss = loss, learning_rate = 0.01)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-a63fe268772d>:33: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-4-a63fe268772d>:43: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From <ipython-input-4-a63fe268772d>:90: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbHtcBDVLesm"
      },
      "source": [
        "class DataReader:\n",
        "  def __init__(self,data_path,batch_size):\n",
        "    self._batch_size = batch_size\n",
        "    with open(data_path) as f:\n",
        "      d_lines = f.read().splitlines()\n",
        "    self._data = []\n",
        "    self._labels = []\n",
        "    self._sentence_lengths = []\n",
        "    self._final_tokens = []\n",
        "    for data_id, line in enumerate(d_lines):\n",
        "      \n",
        "      features = line.split('<fff>')\n",
        "\n",
        "      label, doc_id,length = int(features[0]), int(features[1]), int(features[2])\n",
        "\n",
        "      tokens = [int(tmp) for tmp in  features[3].split()]\n",
        "      \n",
        "      self._data.append(tokens)\n",
        "      self._labels.append(label)\n",
        "      self._sentence_lengths.append(length)\n",
        "      self._final_tokens.append(tokens[length - 1])\n",
        "    self._data = np.array(self._data)\n",
        "    self._labels = np.array(self._labels)\n",
        "    self._sentence_lengths = np.array(self._sentence_lengths)\n",
        "    self._final_tokens = np.array(self._final_tokens)\n",
        "    \n",
        "    self._num_epoch = 0\n",
        "    self._current_part = 0\n",
        "  def next_batch(self):\n",
        "    start = self._current_part * self._batch_size\n",
        "    end = start + self._batch_size\n",
        "    self._current_part += 1\n",
        "\n",
        "    if (end + self._batch_size > len(self._data)):\n",
        "      \n",
        "      self._num_epoch  += 1\n",
        "      self._current_part = 0\n",
        "      indices = range(len(self._data))\n",
        "      random.seed(2021)\n",
        "      random.shuffle(list(indices))\n",
        "      tmpdata = []\n",
        "      tmpy = []\n",
        "      tmplen = []\n",
        "      tmpft = []\n",
        "      tmpdata = self._data[start:end]\n",
        "      tmpy = self._labels[start:end]\n",
        "      tmplen = self._sentence_lengths[start:end] \n",
        "      tmpft = self._final_tokens[start:end]\n",
        "      self._data, self._labels,self._sentence_lengths, self._final_tokens = self._data[indices], self._labels[indices], self._sentence_lengths[indices],self._final_tokens[indices]\n",
        "      return tmpdata,tmpy,tmplen,tmpft\n",
        "    return  self._data[start:end], self._labels[start:end],self._sentence_lengths[start:end], self._final_tokens[start:end]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmI1TRhWauOP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9855e2a4-6bdc-4ca6-a7e3-4ba515b99e33"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  train_data_reader = DataReader(\n",
        "      data_path = '../datasets/20news-bydate/w2v/20news-train-encoded.txt',\n",
        "      batch_size = 50\n",
        "  )\n",
        "  test_data_reader = DataReader(\n",
        "      data_path = '../datasets/20news-bydate/w2v/20news-test-encoded.txt',\n",
        "      batch_size = 50\n",
        "  )\n",
        "  step = 0\n",
        "  MAX_STEP = 10000\n",
        "\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  while step < MAX_STEP:\n",
        "\n",
        "    next_train_batch =  train_data_reader.next_batch()\n",
        "    \n",
        "    train_data, train_labels, train_sentence_lengths, train_final_token = next_train_batch\n",
        "    plabels_eval, loss_eval,_ = sess.run(\n",
        "        [predicted_labels, loss, train_op],\n",
        "        feed_dict = {\n",
        "            rnn._data : train_data,\n",
        "            rnn._labels: train_labels,\n",
        "            rnn._sentence_lengths:train_sentence_lengths,\n",
        "            rnn._final_tokens : train_final_token\n",
        "\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    step += 1\n",
        "    if step % 20 == 0:\n",
        "      print('step: ' + str(step) +' - loss: ', str(loss_eval))\n",
        "\n",
        "    if train_data_reader._current_part == 0:\n",
        "      num_true_preds = 0\n",
        "      while True:\n",
        "        next_test_batch = test_data_reader.next_batch()\n",
        "        test_data, test_labels, test_sentence_lenghts, test_final_tokens = next_test_batch\n",
        "\n",
        "        test_plabels_eval = sess.run(\n",
        "            predicted_labels,\n",
        "            feed_dict = {\n",
        "                rnn._data: test_data,\n",
        "                rnn._labels: test_labels,\n",
        "                rnn._sentence_lengths: test_sentence_lenghts,\n",
        "                rnn._final_tokens: test_final_tokens\n",
        "            }\n",
        "        )\n",
        "        matches = np.equal(test_plabels_eval, test_labels)\n",
        "        num_true_preds += np.sum(matches.astype(float))\n",
        "\n",
        "        if test_data_reader._current_part == 0:\n",
        "          break\n",
        "      \n",
        "      print('Epoch: ', train_data_reader._num_epoch)\n",
        "      print('Accuracy on test data: ', num_true_preds * 100. / len(test_data_reader._data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step: 20 - loss:  0.0012361495\n",
            "step: 40 - loss:  0.49003345\n",
            "step: 60 - loss:  5.757242\n",
            "step: 80 - loss:  0.86787415\n",
            "step: 100 - loss:  3.892943\n",
            "step: 120 - loss:  5.416399\n",
            "step: 140 - loss:  3.0256412\n",
            "step: 160 - loss:  3.6123781\n",
            "step: 180 - loss:  3.6552386\n",
            "step: 200 - loss:  9.024841\n",
            "step: 220 - loss:  3.508162\n",
            "Epoch:  1\n",
            "Accuracy on test data:  5.639031288668906\n",
            "step: 240 - loss:  3.8007042\n",
            "step: 260 - loss:  2.9527574\n",
            "step: 280 - loss:  2.6138244\n",
            "step: 300 - loss:  2.9545527\n",
            "step: 320 - loss:  3.2980201\n",
            "step: 340 - loss:  1.8885423\n",
            "step: 360 - loss:  3.9621894\n",
            "step: 380 - loss:  2.4411442\n",
            "step: 400 - loss:  1.3076682\n",
            "step: 420 - loss:  2.404712\n",
            "step: 440 - loss:  2.4470227\n",
            "Epoch:  2\n",
            "Accuracy on test data:  14.954923104118791\n",
            "step: 460 - loss:  2.2466848\n",
            "step: 480 - loss:  2.2890556\n",
            "step: 500 - loss:  2.504161\n",
            "step: 520 - loss:  1.3569804\n",
            "step: 540 - loss:  2.1602623\n",
            "step: 560 - loss:  2.0577395\n",
            "step: 580 - loss:  1.4307588\n",
            "step: 600 - loss:  2.4023309\n",
            "step: 620 - loss:  2.932227\n",
            "step: 640 - loss:  2.5982342\n",
            "step: 660 - loss:  0.54099727\n",
            "Epoch:  3\n",
            "Accuracy on test data:  31.500795474633197\n",
            "step: 680 - loss:  2.07778\n",
            "step: 700 - loss:  2.2383642\n",
            "step: 720 - loss:  1.8015326\n",
            "step: 740 - loss:  1.5385904\n",
            "step: 760 - loss:  2.2615547\n",
            "step: 780 - loss:  0.97677755\n",
            "step: 800 - loss:  1.0649663\n",
            "step: 820 - loss:  2.4888978\n",
            "step: 840 - loss:  0.75205535\n",
            "step: 860 - loss:  1.2443262\n",
            "step: 880 - loss:  0.9824089\n",
            "step: 900 - loss:  1.779077\n",
            "Epoch:  4\n",
            "Accuracy on test data:  73.82004596075659\n",
            "step: 920 - loss:  1.6436682\n",
            "step: 940 - loss:  1.8079146\n",
            "step: 960 - loss:  1.6436331\n",
            "step: 980 - loss:  0.75136644\n",
            "step: 1000 - loss:  1.2502358\n",
            "step: 1020 - loss:  0.3093245\n",
            "step: 1040 - loss:  0.544116\n",
            "step: 1060 - loss:  1.3364437\n",
            "step: 1080 - loss:  1.0393523\n",
            "step: 1100 - loss:  0.79850024\n",
            "step: 1120 - loss:  0.85540086\n",
            "Epoch:  5\n",
            "Accuracy on test data:  86.26480466678451\n",
            "step: 1140 - loss:  0.71441877\n",
            "step: 1160 - loss:  0.712198\n",
            "step: 1180 - loss:  1.6473111\n",
            "step: 1200 - loss:  0.8290261\n",
            "step: 1220 - loss:  0.52128553\n",
            "step: 1240 - loss:  0.19447799\n",
            "step: 1260 - loss:  0.23331255\n",
            "step: 1280 - loss:  0.39874583\n",
            "step: 1300 - loss:  0.18334532\n",
            "step: 1320 - loss:  0.42062578\n",
            "step: 1340 - loss:  0.41847524\n",
            "Epoch:  6\n",
            "Accuracy on test data:  95.66024394555419\n",
            "step: 1360 - loss:  0.38244492\n",
            "step: 1380 - loss:  0.6540287\n",
            "step: 1400 - loss:  0.7223216\n",
            "step: 1420 - loss:  0.22323665\n",
            "step: 1440 - loss:  0.23747389\n",
            "step: 1460 - loss:  0.10097664\n",
            "step: 1480 - loss:  0.0812949\n",
            "step: 1500 - loss:  0.2662652\n",
            "step: 1520 - loss:  0.13255653\n",
            "step: 1540 - loss:  0.34316823\n",
            "step: 1560 - loss:  0.091128476\n",
            "step: 1580 - loss:  0.3393468\n",
            "Epoch:  7\n",
            "Accuracy on test data:  98.57698426727947\n",
            "step: 1600 - loss:  0.20325764\n",
            "step: 1620 - loss:  0.32747844\n",
            "step: 1640 - loss:  0.28726092\n",
            "step: 1660 - loss:  0.15612026\n",
            "step: 1680 - loss:  0.07602657\n",
            "step: 1700 - loss:  0.082067296\n",
            "step: 1720 - loss:  0.079808354\n",
            "step: 1740 - loss:  0.13454884\n",
            "step: 1760 - loss:  0.08081225\n",
            "step: 1780 - loss:  0.083225146\n",
            "step: 1800 - loss:  0.04167141\n",
            "Epoch:  8\n",
            "Accuracy on test data:  99.38129750751281\n",
            "step: 1820 - loss:  0.23297594\n",
            "step: 1840 - loss:  0.1260733\n",
            "step: 1860 - loss:  0.13161048\n",
            "step: 1880 - loss:  0.07902661\n",
            "step: 1900 - loss:  0.05645708\n",
            "step: 1920 - loss:  0.045465175\n",
            "step: 1940 - loss:  0.06393258\n",
            "step: 1960 - loss:  0.04478213\n",
            "step: 1980 - loss:  0.016470194\n",
            "step: 2000 - loss:  0.037826024\n",
            "step: 2020 - loss:  0.13799362\n",
            "Epoch:  9\n",
            "Accuracy on test data:  99.6906487537564\n",
            "step: 2040 - loss:  0.06554775\n",
            "step: 2060 - loss:  0.1313036\n",
            "step: 2080 - loss:  0.12517355\n",
            "step: 2100 - loss:  0.04033041\n",
            "step: 2120 - loss:  0.066322766\n",
            "step: 2140 - loss:  0.024946116\n",
            "step: 2160 - loss:  0.021144021\n",
            "step: 2180 - loss:  0.0318222\n",
            "step: 2200 - loss:  0.022249982\n",
            "step: 2220 - loss:  0.039058324\n",
            "step: 2240 - loss:  0.011457636\n",
            "step: 2260 - loss:  0.06268588\n",
            "Epoch:  10\n",
            "Accuracy on test data:  99.79671203818278\n",
            "step: 2280 - loss:  0.0514517\n",
            "step: 2300 - loss:  0.07502205\n",
            "step: 2320 - loss:  0.027948981\n",
            "step: 2340 - loss:  0.01880386\n",
            "step: 2360 - loss:  0.014736405\n",
            "step: 2380 - loss:  0.013066585\n",
            "step: 2400 - loss:  0.04900381\n",
            "step: 2420 - loss:  0.013368585\n",
            "step: 2440 - loss:  0.025241202\n",
            "step: 2460 - loss:  0.009158077\n",
            "step: 2480 - loss:  0.05054849\n",
            "Epoch:  11\n",
            "Accuracy on test data:  99.81438925225385\n",
            "step: 2500 - loss:  0.030312462\n",
            "step: 2520 - loss:  0.0756804\n",
            "step: 2540 - loss:  0.031219408\n",
            "step: 2560 - loss:  0.018231623\n",
            "step: 2580 - loss:  0.01412807\n",
            "step: 2600 - loss:  0.007837308\n",
            "step: 2620 - loss:  0.0046056854\n",
            "step: 2640 - loss:  0.0074356575\n",
            "step: 2660 - loss:  0.0032899776\n",
            "step: 2680 - loss:  0.011096209\n",
            "step: 2700 - loss:  0.018424401\n",
            "Epoch:  12\n",
            "Accuracy on test data:  99.82322785928937\n",
            "step: 2720 - loss:  0.019626064\n",
            "step: 2740 - loss:  0.027979784\n",
            "step: 2760 - loss:  0.022773592\n",
            "step: 2780 - loss:  0.016968485\n",
            "step: 2800 - loss:  0.009145013\n",
            "step: 2820 - loss:  0.0027575206\n",
            "step: 2840 - loss:  0.0050626327\n",
            "step: 2860 - loss:  0.013460304\n",
            "step: 2880 - loss:  0.0056167003\n",
            "step: 2900 - loss:  0.01181917\n",
            "step: 2920 - loss:  0.0015028368\n",
            "Epoch:  13\n",
            "Accuracy on test data:  99.81438925225385\n",
            "step: 2940 - loss:  0.010306601\n",
            "step: 2960 - loss:  0.045080956\n",
            "step: 2980 - loss:  0.014393083\n",
            "step: 3000 - loss:  0.026505563\n",
            "step: 3020 - loss:  0.005526894\n",
            "step: 3040 - loss:  0.005622632\n",
            "step: 3060 - loss:  0.0019518249\n",
            "step: 3080 - loss:  0.034199536\n",
            "step: 3100 - loss:  0.0055687153\n",
            "step: 3120 - loss:  0.011234597\n",
            "step: 3140 - loss:  0.005376697\n",
            "step: 3160 - loss:  0.01061242\n",
            "Epoch:  14\n",
            "Accuracy on test data:  99.84974368039597\n",
            "step: 3180 - loss:  0.009521836\n",
            "step: 3200 - loss:  0.011108182\n",
            "step: 3220 - loss:  0.010079776\n",
            "step: 3240 - loss:  0.020400502\n",
            "step: 3260 - loss:  0.0038491276\n",
            "step: 3280 - loss:  0.0015657077\n",
            "step: 3300 - loss:  0.0031036318\n",
            "step: 3320 - loss:  0.0050188657\n",
            "step: 3340 - loss:  0.00424027\n",
            "step: 3360 - loss:  0.0044472106\n",
            "step: 3380 - loss:  0.004196225\n",
            "Epoch:  15\n",
            "Accuracy on test data:  99.84090507336043\n",
            "step: 3400 - loss:  0.014866169\n",
            "step: 3420 - loss:  0.010186417\n",
            "step: 3440 - loss:  0.0072138826\n",
            "step: 3460 - loss:  0.00528968\n",
            "step: 3480 - loss:  0.0031535083\n",
            "step: 3500 - loss:  0.0016971789\n",
            "step: 3520 - loss:  0.0009965475\n",
            "step: 3540 - loss:  0.004468881\n",
            "step: 3560 - loss:  0.0010461449\n",
            "step: 3580 - loss:  0.003004948\n",
            "step: 3600 - loss:  0.0030382636\n",
            "Epoch:  16\n",
            "Accuracy on test data:  99.84974368039597\n",
            "step: 3620 - loss:  0.005318952\n",
            "step: 3640 - loss:  0.006965818\n",
            "step: 3660 - loss:  0.02682084\n",
            "step: 3680 - loss:  0.0034604294\n",
            "step: 3700 - loss:  0.0024882508\n",
            "step: 3720 - loss:  0.0015354495\n",
            "step: 3740 - loss:  0.0014267691\n",
            "step: 3760 - loss:  0.0027885905\n",
            "step: 3780 - loss:  0.002483165\n",
            "step: 3800 - loss:  0.0047629783\n",
            "step: 3820 - loss:  0.0017374542\n",
            "step: 3840 - loss:  0.005000509\n",
            "Epoch:  17\n",
            "Accuracy on test data:  99.8585822874315\n",
            "step: 3860 - loss:  0.0069433223\n",
            "step: 3880 - loss:  0.0059084534\n",
            "step: 3900 - loss:  0.00626722\n",
            "step: 3920 - loss:  0.0028160242\n",
            "step: 3940 - loss:  0.0017161501\n",
            "step: 3960 - loss:  0.0017019687\n",
            "step: 3980 - loss:  0.0015118051\n",
            "step: 4000 - loss:  0.0022839375\n",
            "step: 4020 - loss:  0.0023032445\n",
            "step: 4040 - loss:  0.0022753973\n",
            "step: 4060 - loss:  0.0010164104\n",
            "Epoch:  18\n",
            "Accuracy on test data:  99.8585822874315\n",
            "step: 4080 - loss:  0.004782969\n",
            "step: 4100 - loss:  0.0029477405\n",
            "step: 4120 - loss:  0.0034691363\n",
            "step: 4140 - loss:  0.0018163723\n",
            "step: 4160 - loss:  0.0016891658\n",
            "step: 4180 - loss:  0.0012987071\n",
            "step: 4200 - loss:  0.0010960262\n",
            "step: 4220 - loss:  0.0013653861\n",
            "step: 4240 - loss:  0.000456479\n",
            "step: 4260 - loss:  0.0016438961\n",
            "step: 4280 - loss:  0.0024779595\n",
            "Epoch:  19\n",
            "Accuracy on test data:  99.8585822874315\n",
            "step: 4300 - loss:  0.0022882305\n",
            "step: 4320 - loss:  0.003704766\n",
            "step: 4340 - loss:  0.003827669\n",
            "step: 4360 - loss:  0.0018668532\n",
            "step: 4380 - loss:  0.001804555\n",
            "step: 4400 - loss:  0.00092145614\n",
            "step: 4420 - loss:  0.000747401\n",
            "step: 4440 - loss:  0.0011739663\n",
            "step: 4460 - loss:  0.0008043967\n",
            "step: 4480 - loss:  0.0016595896\n",
            "step: 4500 - loss:  0.00050220086\n",
            "step: 4520 - loss:  0.0029910335\n",
            "Epoch:  20\n",
            "Accuracy on test data:  99.86742089446703\n",
            "step: 4540 - loss:  0.0020015151\n",
            "step: 4560 - loss:  0.002446672\n",
            "step: 4580 - loss:  0.001433896\n",
            "step: 4600 - loss:  0.002666084\n",
            "step: 4620 - loss:  0.0007298609\n",
            "step: 4640 - loss:  0.0005661335\n",
            "step: 4660 - loss:  0.001441682\n",
            "step: 4680 - loss:  0.0008679472\n",
            "step: 4700 - loss:  0.0010401977\n",
            "step: 4720 - loss:  0.0005244696\n",
            "step: 4740 - loss:  0.002392066\n",
            "Epoch:  21\n",
            "Accuracy on test data:  99.86742089446703\n",
            "step: 4760 - loss:  0.0014404643\n",
            "step: 4780 - loss:  0.00426337\n",
            "step: 4800 - loss:  0.001654714\n",
            "step: 4820 - loss:  0.0011961482\n",
            "step: 4840 - loss:  0.0011991754\n",
            "step: 4860 - loss:  0.0006880567\n",
            "step: 4880 - loss:  0.00031793225\n",
            "step: 4900 - loss:  0.00050739927\n",
            "step: 4920 - loss:  0.00029079494\n",
            "step: 4940 - loss:  0.00072589697\n",
            "step: 4960 - loss:  0.0011179037\n",
            "Epoch:  22\n",
            "Accuracy on test data:  99.86742089446703\n",
            "step: 4980 - loss:  0.00103436\n",
            "step: 5000 - loss:  0.0014787257\n",
            "step: 5020 - loss:  0.0013603662\n",
            "step: 5040 - loss:  0.001353442\n",
            "step: 5060 - loss:  0.00051146804\n",
            "step: 5080 - loss:  0.0003336075\n",
            "step: 5100 - loss:  0.0003652023\n",
            "step: 5120 - loss:  0.0010350393\n",
            "step: 5140 - loss:  0.00042989154\n",
            "step: 5160 - loss:  0.00082107\n",
            "step: 5180 - loss:  0.0001508784\n",
            "Epoch:  23\n",
            "Accuracy on test data:  99.8585822874315\n",
            "step: 5200 - loss:  0.00085148023\n",
            "step: 5220 - loss:  0.024442364\n",
            "step: 5240 - loss:  0.00096858316\n",
            "step: 5260 - loss:  0.0085108075\n",
            "step: 5280 - loss:  0.000578899\n",
            "step: 5300 - loss:  0.00048403325\n",
            "step: 5320 - loss:  0.00018286532\n",
            "step: 5340 - loss:  0.012419897\n",
            "step: 5360 - loss:  0.0005072608\n",
            "step: 5380 - loss:  0.0009025752\n",
            "step: 5400 - loss:  0.00046274308\n",
            "step: 5420 - loss:  0.001075173\n",
            "Epoch:  24\n",
            "Accuracy on test data:  99.86742089446703\n",
            "step: 5440 - loss:  0.0009549673\n",
            "step: 5460 - loss:  0.0011490268\n",
            "step: 5480 - loss:  0.0010848539\n",
            "step: 5500 - loss:  0.009117622\n",
            "step: 5520 - loss:  0.00041935098\n",
            "step: 5540 - loss:  0.00018024609\n",
            "step: 5560 - loss:  0.00036508698\n",
            "step: 5580 - loss:  0.0005050971\n",
            "step: 5600 - loss:  0.000543893\n",
            "step: 5620 - loss:  0.0004439214\n",
            "step: 5640 - loss:  0.00046728802\n",
            "Epoch:  25\n",
            "Accuracy on test data:  99.86742089446703\n",
            "step: 5660 - loss:  0.010809099\n",
            "step: 5680 - loss:  0.00093215774\n",
            "step: 5700 - loss:  0.00077928527\n",
            "step: 5720 - loss:  0.0006138062\n",
            "step: 5740 - loss:  0.0003284044\n",
            "step: 5760 - loss:  0.0002022355\n",
            "step: 5780 - loss:  0.00016479548\n",
            "step: 5800 - loss:  0.000399451\n",
            "step: 5820 - loss:  0.00016569249\n",
            "step: 5840 - loss:  0.0003726216\n",
            "step: 5860 - loss:  0.0004293682\n",
            "Epoch:  26\n",
            "Accuracy on test data:  99.86742089446703\n",
            "step: 5880 - loss:  0.0005534529\n",
            "step: 5900 - loss:  0.00075714494\n",
            "step: 5920 - loss:  0.010053737\n",
            "step: 5940 - loss:  0.00041650995\n",
            "step: 5960 - loss:  0.00033024538\n",
            "step: 5980 - loss:  0.00024456193\n",
            "step: 6000 - loss:  0.00018185911\n",
            "step: 6020 - loss:  0.00036724415\n",
            "step: 6040 - loss:  0.000302018\n",
            "step: 6060 - loss:  0.00059342204\n",
            "step: 6080 - loss:  0.00028676828\n",
            "step: 6100 - loss:  0.0006675587\n",
            "Epoch:  27\n",
            "Accuracy on test data:  99.86742089446703\n",
            "step: 6120 - loss:  0.0021493\n",
            "step: 6140 - loss:  0.0007296436\n",
            "step: 6160 - loss:  0.00070822\n",
            "step: 6180 - loss:  0.0004210891\n",
            "step: 6200 - loss:  0.00018892925\n",
            "step: 6220 - loss:  0.00025581365\n",
            "step: 6240 - loss:  0.0002200611\n",
            "step: 6260 - loss:  0.0003793204\n",
            "step: 6280 - loss:  0.00035970155\n",
            "step: 6300 - loss:  0.0003436897\n",
            "step: 6320 - loss:  0.00016641\n",
            "Epoch:  28\n",
            "Accuracy on test data:  99.86742089446703\n",
            "step: 6340 - loss:  0.00066583214\n",
            "step: 6360 - loss:  0.00042738952\n",
            "step: 6380 - loss:  0.0005388398\n",
            "step: 6400 - loss:  0.0003364194\n",
            "step: 6420 - loss:  0.00024176527\n",
            "step: 6440 - loss:  0.00021641744\n",
            "step: 6460 - loss:  0.00019186424\n",
            "step: 6480 - loss:  0.0002274959\n",
            "step: 6500 - loss:  9.315295e-05\n",
            "step: 6520 - loss:  0.0002895405\n",
            "step: 6540 - loss:  0.00036417937\n",
            "Epoch:  29\n",
            "Accuracy on test data:  99.86742089446703\n",
            "step: 6560 - loss:  0.00039608625\n",
            "step: 6580 - loss:  0.00059650175\n",
            "step: 6600 - loss:  0.0006687462\n",
            "step: 6620 - loss:  0.00031817507\n",
            "step: 6640 - loss:  0.00028629552\n",
            "step: 6660 - loss:  0.00023199979\n",
            "step: 6680 - loss:  0.00012718205\n",
            "step: 6700 - loss:  0.00023955539\n",
            "step: 6720 - loss:  0.00018170418\n",
            "step: 6740 - loss:  0.00029367366\n",
            "step: 6760 - loss:  0.0001073147\n",
            "step: 6780 - loss:  0.00061508996\n",
            "Epoch:  30\n",
            "Accuracy on test data:  99.86742089446703\n",
            "step: 6800 - loss:  0.00050677196\n",
            "step: 6820 - loss:  0.00044067355\n",
            "step: 6840 - loss:  0.0003176096\n",
            "step: 6860 - loss:  0.0015895206\n",
            "step: 6880 - loss:  0.00015471257\n",
            "step: 6900 - loss:  0.00012616119\n",
            "step: 6920 - loss:  0.00034331885\n",
            "step: 6940 - loss:  0.00016597421\n",
            "step: 6960 - loss:  0.00026796345\n",
            "step: 6980 - loss:  0.00010338215\n",
            "step: 7000 - loss:  0.00048301672\n",
            "Epoch:  31\n",
            "Accuracy on test data:  99.86742089446703\n",
            "step: 7020 - loss:  0.0003397654\n",
            "step: 7040 - loss:  0.0009103966\n",
            "step: 7060 - loss:  0.00034244847\n",
            "step: 7080 - loss:  0.00028352672\n",
            "step: 7100 - loss:  0.0007352993\n",
            "step: 7120 - loss:  0.00014804781\n",
            "step: 7140 - loss:  7.1169365e-05\n",
            "step: 7160 - loss:  0.0001175342\n",
            "step: 7180 - loss:  6.0260114e-05\n",
            "step: 7200 - loss:  0.00019055704\n",
            "step: 7220 - loss:  0.00021058723\n",
            "Epoch:  32\n",
            "Accuracy on test data:  99.86742089446703\n",
            "step: 7240 - loss:  0.0002545162\n",
            "step: 7260 - loss:  0.00032713718\n",
            "step: 7280 - loss:  0.00030228856\n",
            "step: 7300 - loss:  0.00026478822\n",
            "step: 7320 - loss:  0.00011645725\n",
            "step: 7340 - loss:  7.5455806e-05\n",
            "step: 7360 - loss:  0.00010780365\n",
            "step: 7380 - loss:  0.00020937626\n",
            "step: 7400 - loss:  0.00011404678\n",
            "step: 7420 - loss:  0.0002184548\n",
            "step: 7440 - loss:  5.433665e-05\n",
            "Epoch:  33\n",
            "Accuracy on test data:  99.86742089446703\n",
            "step: 7460 - loss:  0.0002107067\n",
            "step: 7480 - loss:  0.03374734\n",
            "step: 7500 - loss:  0.00023785437\n",
            "step: 7520 - loss:  0.004374902\n",
            "step: 7540 - loss:  0.00013600032\n",
            "step: 7560 - loss:  0.000119900884\n",
            "step: 7580 - loss:  6.110079e-05\n",
            "step: 7600 - loss:  0.0066024745\n",
            "step: 7620 - loss:  0.00013153098\n",
            "step: 7640 - loss:  0.0002313455\n",
            "step: 7660 - loss:  0.00010673714\n",
            "step: 7680 - loss:  0.0003413001\n",
            "Epoch:  34\n",
            "Accuracy on test data:  99.8585822874315\n",
            "step: 7700 - loss:  0.00030442612\n",
            "step: 7720 - loss:  0.00040903484\n",
            "step: 7740 - loss:  0.0003494373\n",
            "step: 7760 - loss:  0.030424833\n",
            "step: 7780 - loss:  0.00014371067\n",
            "step: 7800 - loss:  9.231642e-05\n",
            "step: 7820 - loss:  0.00021987065\n",
            "step: 7840 - loss:  0.00020931156\n",
            "step: 7860 - loss:  0.00060482405\n",
            "step: 7880 - loss:  0.00033223917\n",
            "step: 7900 - loss:  0.00029857046\n",
            "Epoch:  35\n",
            "Accuracy on test data:  99.84974368039597\n",
            "step: 7920 - loss:  0.015963797\n",
            "step: 7940 - loss:  0.0003109982\n",
            "step: 7960 - loss:  0.0007453941\n",
            "step: 7980 - loss:  0.0228705\n",
            "step: 8000 - loss:  9.536742e-09\n",
            "step: 8020 - loss:  1.485654e-05\n",
            "step: 8040 - loss:  2.2005978\n",
            "step: 8060 - loss:  0.0003192745\n",
            "step: 8080 - loss:  0.07575588\n",
            "step: 8100 - loss:  1.9950737\n",
            "step: 8120 - loss:  2.0689938\n",
            "Epoch:  36\n",
            "Accuracy on test data:  33.524836485769846\n",
            "step: 8140 - loss:  1.5673866\n",
            "step: 8160 - loss:  2.8686042\n",
            "step: 8180 - loss:  0.4737958\n",
            "step: 8200 - loss:  0.6755019\n",
            "step: 8220 - loss:  1.6337506\n",
            "step: 8240 - loss:  0.40531746\n",
            "step: 8260 - loss:  0.29925996\n",
            "step: 8280 - loss:  1.0753293\n",
            "step: 8300 - loss:  0.5939109\n",
            "step: 8320 - loss:  0.36131588\n",
            "step: 8340 - loss:  0.47462228\n",
            "step: 8360 - loss:  0.77180064\n",
            "Epoch:  37\n",
            "Accuracy on test data:  77.55877673678629\n",
            "step: 8380 - loss:  0.96869004\n",
            "step: 8400 - loss:  1.2649409\n",
            "step: 8420 - loss:  0.7464144\n",
            "step: 8440 - loss:  0.61718416\n",
            "step: 8460 - loss:  0.59956217\n",
            "step: 8480 - loss:  0.4255571\n",
            "step: 8500 - loss:  0.38312203\n",
            "step: 8520 - loss:  0.5653967\n",
            "step: 8540 - loss:  0.414635\n",
            "step: 8560 - loss:  0.28427735\n",
            "step: 8580 - loss:  0.12391772\n",
            "Epoch:  38\n",
            "Accuracy on test data:  95.52766484002122\n",
            "step: 8600 - loss:  0.60701203\n",
            "step: 8620 - loss:  0.2719043\n",
            "step: 8640 - loss:  0.5179006\n",
            "step: 8660 - loss:  0.20887034\n",
            "step: 8680 - loss:  0.33039\n",
            "step: 8700 - loss:  0.24403973\n",
            "step: 8720 - loss:  0.0512734\n",
            "step: 8740 - loss:  0.20327975\n",
            "step: 8760 - loss:  0.027031139\n",
            "step: 8780 - loss:  0.06408444\n",
            "step: 8800 - loss:  0.15989803\n",
            "Epoch:  39\n",
            "Accuracy on test data:  98.7802722290967\n",
            "step: 8820 - loss:  0.107017145\n",
            "step: 8840 - loss:  0.13880698\n",
            "step: 8860 - loss:  0.19524401\n",
            "step: 8880 - loss:  0.055589657\n",
            "step: 8900 - loss:  0.15049091\n",
            "step: 8920 - loss:  0.068353236\n",
            "step: 8940 - loss:  0.05520753\n",
            "step: 8960 - loss:  0.14962998\n",
            "step: 8980 - loss:  0.037566673\n",
            "step: 9000 - loss:  0.08408268\n",
            "step: 9020 - loss:  0.032377373\n",
            "step: 9040 - loss:  0.1502672\n",
            "Epoch:  40\n",
            "Accuracy on test data:  99.19568675976666\n",
            "step: 9060 - loss:  0.1878807\n",
            "step: 9080 - loss:  0.11518392\n",
            "step: 9100 - loss:  0.087802455\n",
            "step: 9120 - loss:  0.05496585\n",
            "step: 9140 - loss:  0.06469598\n",
            "step: 9160 - loss:  0.014820187\n",
            "step: 9180 - loss:  0.16311474\n",
            "step: 9200 - loss:  0.07840553\n",
            "step: 9220 - loss:  0.030051734\n",
            "step: 9240 - loss:  0.022147374\n",
            "step: 9260 - loss:  0.11792302\n",
            "Epoch:  41\n",
            "Accuracy on test data:  99.49619939897472\n",
            "step: 9280 - loss:  0.03666471\n",
            "step: 9300 - loss:  0.21046896\n",
            "step: 9320 - loss:  0.045762725\n",
            "step: 9340 - loss:  0.05232709\n",
            "step: 9360 - loss:  0.05445698\n",
            "step: 9380 - loss:  0.088578455\n",
            "step: 9400 - loss:  0.020105973\n",
            "step: 9420 - loss:  0.017007114\n",
            "step: 9440 - loss:  0.008264023\n",
            "step: 9460 - loss:  0.037460785\n",
            "step: 9480 - loss:  0.048269663\n",
            "Epoch:  42\n",
            "Accuracy on test data:  99.59342407636557\n",
            "step: 9500 - loss:  0.038085476\n",
            "step: 9520 - loss:  0.05981418\n",
            "step: 9540 - loss:  0.057139896\n",
            "step: 9560 - loss:  0.02843912\n",
            "step: 9580 - loss:  0.035218142\n",
            "step: 9600 - loss:  0.019349074\n",
            "step: 9620 - loss:  0.01587142\n",
            "step: 9640 - loss:  0.15569128\n",
            "step: 9660 - loss:  0.020926235\n",
            "step: 9680 - loss:  0.025191637\n",
            "step: 9700 - loss:  0.0054469723\n",
            "Epoch:  43\n",
            "Accuracy on test data:  99.66413293264982\n",
            "step: 9720 - loss:  0.0179434\n",
            "step: 9740 - loss:  0.064370215\n",
            "step: 9760 - loss:  0.033112913\n",
            "step: 9780 - loss:  0.026716933\n",
            "step: 9800 - loss:  0.024694178\n",
            "step: 9820 - loss:  0.036901765\n",
            "step: 9840 - loss:  0.008804166\n",
            "step: 9860 - loss:  0.08954004\n",
            "step: 9880 - loss:  0.059242934\n",
            "step: 9900 - loss:  0.03258475\n",
            "step: 9920 - loss:  0.034058608\n",
            "step: 9940 - loss:  0.028342126\n",
            "Epoch:  44\n",
            "Accuracy on test data:  99.717164574863\n",
            "step: 9960 - loss:  0.025740808\n",
            "step: 9980 - loss:  0.04485785\n",
            "step: 10000 - loss:  0.041248206\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}